{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from datasets.create_datasets import (\n",
    "    create_frame_dataset,\n",
    "    create_patch_dataset,\n",
    ")\n",
    "\n",
    "from detection_utils import (\n",
    "    accept_patch,\n",
    "    add_and_pop,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "mtcnn_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # MTCNN only supports CUDA or CPU\n",
    "\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection, segmentation, and pose models\n",
    "os.makedirs('./pretrained-models', exist_ok=True)\n",
    "\n",
    "classification_model = YOLO('./pretrained-models/yolov8m.pt')\n",
    "segmentation_model = YOLO('./pretrained-models/yolov8l-seg.pt')\n",
    "pose_model = YOLO('./pretrained-models/yolov8l-pose.pt')\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True, device=mtcnn_device, image_size=256, margin=8, thresholds=[0.5,0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_human_patches(batch, lookback, filter=True):\n",
    "    HUMAN_CLASS = 0\n",
    "    CONFIDENCE_THRESHOLD = 0.8 if filter else 0.35\n",
    "\n",
    "    patches = []\n",
    "    with torch.no_grad():\n",
    "        results = classification_model(batch, verbose=False)\n",
    "        assert len(results) == batch.shape[0]\n",
    "\n",
    "        for frame, r in enumerate(results):\n",
    "            for box in r.boxes:\n",
    "                if box.cls ==  HUMAN_CLASS and box.conf > CONFIDENCE_THRESHOLD:\n",
    "                    x1, y1, x2, y2 = box.xyxy.int().tolist()[0]\n",
    "                    patch = batch[frame, :, y1:y2, x1:x2]\n",
    "\n",
    "                    if filter:\n",
    "                        if accept_patch(patch, lookback):\n",
    "                            patches.append(patch.unsqueeze(0))\n",
    "                            add_and_pop(lookback, patch)\n",
    "                    else:\n",
    "                        patches.append({\n",
    "                            'patch': patch,\n",
    "                            'coords': (x1, y1, x2, y2),\n",
    "                        })\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = ['/game/MafiaVideogame','/movie/TheGodfather', '/movie/TheSopranos', '/movie/TheIrishman']\n",
    "\n",
    "for video in videos:\n",
    "    video_path = f'Data/Train/{video}.mp4'\n",
    "    save_dir = f'Dataset/patches/{video.split(\"/\")[-1]}'\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if len(os.listdir(save_dir)) > 0:\n",
    "        continue\n",
    "\n",
    "    dataset = create_frame_dataset(video_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    patch_count = 0\n",
    "    lookback = []\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = batch.to(device)\n",
    "        batch_to_list = lambda x: [x[i] for i in range(len(x))]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # First, detect objects in the frame\n",
    "            human_patches = extract_human_patches(batch, lookback)\n",
    "\n",
    "            for patch in human_patches:\n",
    "                save_image(patch, os.path.join(save_dir, f'patch_{patch_count}.png'))\n",
    "                patch_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Example Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_dict(batch):\n",
    "    return {\n",
    "        'patch': torch.stack([x['patch'] for x in batch]),\n",
    "        'patch_path': np.array([x['patch_path'] for x in batch])\n",
    "    }\n",
    "\n",
    "train_dataset = create_patch_dataset('Dataset/patches')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_dict)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch_2 = next(iter(train_dataloader))\n",
    "\n",
    "patches = torch.cat([batch['patch'], batch_2['patch']])\n",
    "\n",
    "# Plot 50 random patches\n",
    "rows, columns = 5, 10\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "idx = 0\n",
    "for i in range(1, rows * columns + 1):\n",
    "    plt.subplot(rows, columns, i)\n",
    "    plt.imshow(patches[idx].permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    idx += 1\n",
    "\n",
    "plt.suptitle('1.1 Human Patch Extraction - 50 Randomly Sampled Patches', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use YOLO to segment objects\n",
    "def detect_and_save_segmentation(batch, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    patches, patch_paths = batch['patch'], batch['patch_path']\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict poses and save to disk\n",
    "        results = segmentation_model(patches, verbose=False)\n",
    "        assert len(results) == patches.shape[0]\n",
    "\n",
    "        for result, patch_path in zip(results, patch_paths):\n",
    "            masks = result.masks\n",
    "\n",
    "            if masks == None:\n",
    "                print(f\"No masks detected for {patch_path}\")\n",
    "                continue\n",
    "\n",
    "            cls = result.boxes.cls\n",
    "            conf = result.boxes.conf\n",
    "\n",
    "            masks = masks[cls == 0]\n",
    "            conf = conf[cls == 0]\n",
    "\n",
    "            data = {\n",
    "                'masks': masks,\n",
    "                'conf': conf,\n",
    "            }\n",
    "\n",
    "            save_path = os.path.join(save_dir, os.path.basename(patch_path).replace('.png', '.pt'))\n",
    "            torch.save(data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = ['TheGodfather', 'TheSopranos', 'TheIrishman', 'MafiaVideogame']\n",
    "\n",
    "for video in videos:\n",
    "    patches_dir = f'Dataset/patches/{video}'\n",
    "    save_dir = f'Dataset/segmentations/{video}'\n",
    "\n",
    "    if len(os.listdir(save_dir)) > 0:\n",
    "        continue\n",
    "\n",
    "    dataset = create_patch_dataset(patches_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        detect_and_save_segmentation(batch, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset Curation\n",
    "\n",
    "- We currently have a selection of patches with positive human detections, as well as segmentation masks\n",
    "- We use the segmentation masks to divide a patch into their respective human pixels, and save these separately\n",
    "- Finally, we will apply face & pose detection to each segmented patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_dir = 'Dataset/patches'\n",
    "\n",
    "os.makedirs('DatasetProcessed/', exist_ok=True)\n",
    "os.makedirs('DatasetProcessed/train_A', exist_ok=True)\n",
    "os.makedirs('DatasetProcessed/train_B', exist_ok=True)\n",
    "os.makedirs('DatasetProcessed/test_A', exist_ok=True)\n",
    "os.makedirs('DatasetProcessed/test_B', exist_ok=True)\n",
    "\n",
    "videos = ['TheGodfather', 'TheSopranos', 'TheIrishman', 'MafiaVideogame']\n",
    "\n",
    "final_patches = []\n",
    "conf_rejected_patches = []\n",
    "rejected_patches = []\n",
    "for video in videos:\n",
    "    suffix = 'A' if video in ['TheGodfather', 'TheSopranos', 'TheIrishman'] else 'B'\n",
    "\n",
    "    dataset = create_patch_dataset(f'Dataset/patches/{video}')\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        patch = batch['patch'][0]\n",
    "        patch_path = batch['patch_path'][0]\n",
    "        segmentation_path = patch_path.replace('patches', 'segmentations').replace('.png', '.pt')\n",
    "\n",
    "        if not os.path.exists(segmentation_path):\n",
    "            continue\n",
    "\n",
    "        # === Open Segmentation ===\n",
    "        segmentation = torch.load(segmentation_path)\n",
    "        masks, conf = segmentation['masks'], segmentation['conf']\n",
    "        masks = masks.data\n",
    "        num_segmentations = masks.shape[0]\n",
    "\n",
    "        # If there are multiple segmentations, we split the patch into multiple patches\n",
    "        for i in range(num_segmentations):\n",
    "            mask = masks[i].unsqueeze(0)\n",
    "            mask_conf = conf[i]\n",
    "\n",
    "            if mask_conf < 0.75:\n",
    "                continue\n",
    "            mask = mask.repeat(3, 1, 1)\n",
    "            masked_patch = patch * mask\n",
    "\n",
    "            final_patches.append((masked_patch, suffix))\n",
    "\n",
    "print(f'[INFO] Accepted total of {len(final_patches)} patches')\n",
    "\n",
    "# Split into train and test\n",
    "train_patches, test_patches = train_test_split(final_patches, test_size=0.2)\n",
    "\n",
    "idx = 0\n",
    "for (patch, suffix) in train_patches:\n",
    "    save_image(patch, f'DatasetProcessed/train_{suffix}/patch_{idx}.png')\n",
    "    idx += 1\n",
    "\n",
    "for (patch, suffix) in test_patches:\n",
    "    save_image(patch, f'DatasetProcessed/test_{suffix}/patch_{idx}.png')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Pose Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to manually keep map indices to their corresponding limbs, sigh...\n",
    "\n",
    "# Classify into\n",
    "# 1) Full-body front/back view\n",
    "# 2) Head-and-shoulder front/back view\n",
    "# 3) Others\n",
    "\n",
    "POSE_MAP = {\n",
    "    # Face\n",
    "    0: 'Nose',\n",
    "    1: 'Left Eye',\n",
    "    2: 'Right Eye',\n",
    "    3: 'Left Ear',\n",
    "    4: 'Right Ear',\n",
    "    # Upper\n",
    "    5: 'Left Shoulder',\n",
    "    6: 'Right Shoulder',\n",
    "    7: 'Left Elbow',\n",
    "    8: 'Right Elbow',\n",
    "    9: 'Left Hand',\n",
    "    10: 'Right Hand',\n",
    "    # Lower\n",
    "    11: 'Left Hip',\n",
    "    12: 'Right Hip',\n",
    "    13: 'Left Leg',\n",
    "    14: 'Right Leg',\n",
    "    15: 'Left Foot',\n",
    "    16: 'Right Foot',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MTCNN to detect faces and landmarks\n",
    "def detect_and_save_faces(batch, save_dir):\n",
    "    # Patch is [B, C, H, W], we have to process each individually as [H, W, C]\n",
    "    patches, patch_path = batch['patch'], batch['patch_path']\n",
    "    batch_size = patches.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        patch_i = patches[i]\n",
    "        patch_path_i = patch_path[i]\n",
    "\n",
    "        # Convert to Numpy array of shape (H, W, C)\n",
    "        patch_i = patch_i.detach().cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "\n",
    "        # Convert to PIL\n",
    "        patch_i = (patch_i * 255).astype(np.uint8)\n",
    "        patch_i = Image.fromarray(patch_i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            boxes, landmarks = mtcnn.detect(patch_i)\n",
    "\n",
    "            save_object = {\n",
    "                'boxes': boxes,\n",
    "                'landmarks': landmarks,\n",
    "            }\n",
    "\n",
    "            save_path = os.path.join(save_dir, os.path.basename(patch_path_i).replace('.png', '.npy'))\n",
    "            np.save(save_path, save_object)\n",
    "\n",
    "# Use YOLO to detect poses\n",
    "def detect_and_save_poses(batch, save_dir):\n",
    "    patches, patch_paths = batch['patch'], batch['patch_path']\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict poses and save to disk\n",
    "        results = pose_model(patches, verbose=False)\n",
    "        assert len(results) == patches.shape[0]\n",
    "\n",
    "        for result, patch_path in zip(results, patch_paths):\n",
    "            # Shape [1, 17, 2]\n",
    "            keypoints_xy = result.keypoints.xy\n",
    "            keypoints_xy = keypoints_xy.squeeze().cpu()\n",
    "\n",
    "            # Save as .pt\n",
    "            save_path = os.path.join(save_dir, os.path.basename(patch_path).replace('.png', '.pt'))\n",
    "            torch.save(keypoints_xy, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_dir = './DatasetProcessed'\n",
    "dirs = ['/train_A', '/train_B', '/test_A', '/test_B']\n",
    "\n",
    "faces_dir = 'DatasetProcessed/faces'\n",
    "poses_dir = 'DatasetProcessed/poses'\n",
    "\n",
    "if not os.path.exists(faces_dir) and not os.path.exists(poses_dir):\n",
    "    for dir in dirs:\n",
    "        dataset = create_patch_dataset(f'{patches_dir}/{dir}')\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            detect_and_save_faces(batch, faces_dir)\n",
    "            detect_and_save_poses(batch, poses_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose classification and final filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enums for classification\n",
    "FRONT_HEAD_AND_SHOULDERS = 0\n",
    "BACK_HEAD_AND_SHOULDERS = 1\n",
    "\n",
    "FRONT_FULL_BODY = 2\n",
    "BACK_FULL_BODY = 3\n",
    "\n",
    "OTHER = 4\n",
    "\n",
    "def classify_patch(face, pose):\n",
    "    if pose.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    # If we detected multiple poses, we take the first one\n",
    "    if pose.ndim == 3:\n",
    "        pose = pose[0]\n",
    "\n",
    "    # Map pose indices to body parts\n",
    "    pose_mapped = {\n",
    "        v: pose[k] for k, v in POSE_MAP.items() if pose[k].sum() > 0\n",
    "    }\n",
    "\n",
    "    face_markers = [body_part in pose_mapped for body_part in ['Nose', 'Left Eye', 'Right Eye', 'Left Ear', 'Right Ear']]\n",
    "    head_and_shoulders_markers = [body_part in pose_mapped for body_part in ['Left Shoulder', 'Right Shoulder', 'Left Ear', 'Right Ear']]\n",
    "    full_body_markers = [body_part in pose_mapped for body_part in ['Left Hip', 'Right Hip', 'Left Leg', 'Right Leg', 'Left Foot', 'Right Foot']]\n",
    "    \n",
    "    # Perfect front view will have both MTCNN positive detection and all 5 face markers\n",
    "    front_view = (face is not None) and (sum(face_markers) >= 4)\n",
    "    \n",
    "    # Side view will have at least 3 face markers\n",
    "    side_profile = sum(face_markers) >= 3\n",
    "\n",
    "    # H&S should have torso markers\n",
    "    head_and_shoulders = sum(head_and_shoulders_markers) >= len(head_and_shoulders_markers) // 2\n",
    "\n",
    "    # Full body should have leg markers\n",
    "    full_body = sum(full_body_markers) >= len(full_body_markers) // 3\n",
    "\n",
    "    if front_view:\n",
    "        if full_body:\n",
    "            return FRONT_FULL_BODY\n",
    "        elif head_and_shoulders:\n",
    "            return FRONT_HEAD_AND_SHOULDERS\n",
    "        else:\n",
    "            return None\n",
    "    elif side_profile:\n",
    "        return OTHER\n",
    "    else:\n",
    "        if full_body:\n",
    "            return BACK_FULL_BODY\n",
    "        elif head_and_shoulders:\n",
    "            return BACK_HEAD_AND_SHOULDERS\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_dir = './DatasetProcessed'\n",
    "dirs = ['/train_A', '/train_B', '/test_A', '/test_B']\n",
    "\n",
    "classifications = {}\n",
    "for dir in dirs:\n",
    "    dataset = create_patch_dataset(f'{patches_dir}/{dir}')\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        patch = batch['patch'][0]\n",
    "        patch_path = batch['patch_path'][0]\n",
    "        face_path = patch_path.replace(dir, 'faces').replace('.png', '.npy')\n",
    "        pose_path = patch_path.replace(dir, 'poses').replace('.png', '.pt')\n",
    "\n",
    "        percent_active = (patch > 0).sum().item() / patch.numel()\n",
    "        if percent_active < 0.15:\n",
    "            os.remove(patch_path)\n",
    "            os.remove(face_path)\n",
    "            os.remove(pose_path)\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(face_path) or not os.path.exists(pose_path):\n",
    "            os.remove(patch_path)\n",
    "            continue\n",
    "\n",
    "        face = np.load(face_path, allow_pickle=True).item()\n",
    "        pose = torch.load(pose_path)\n",
    "\n",
    "        classification = classify_patch(face, pose)\n",
    "\n",
    "        if classification is not None:\n",
    "            classifications[patch_path] = classification\n",
    "\n",
    "# Save to JSON\n",
    "classifications_json_path = 'DatasetProcessed/classifications.json'\n",
    "\n",
    "with open(classifications_json_path, 'w') as f:\n",
    "    json.dump(classifications, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle classifications\n",
    "classification_english = {\n",
    "    0: 'Front Head-and-Shoulders',\n",
    "    1: 'Back Head-and-Shoulders',\n",
    "    2: 'Front Full Body',\n",
    "    3: 'Back Full Body',\n",
    "    4: 'Other'\n",
    "}\n",
    "\n",
    "classifications = dict(sorted(classifications.items(), key=lambda x: np.random.rand()))\n",
    "\n",
    "plot_images = {k: [] for k in range(5)}\n",
    "\n",
    "for i, (patch_path, classification) in enumerate(classifications.items()):\n",
    "    if len(plot_images[classification]) >= 25:\n",
    "        continue\n",
    "\n",
    "    patch = Image.open(patch_path)\n",
    "    plot_images[classification].append(patch)\n",
    "\n",
    "    if all([len(images) >= 25 for images in plot_images.values()]):\n",
    "        break\n",
    "\n",
    "# For each pose, make a plot of 25 images with the title of the pose\n",
    "for i, (classification, images) in enumerate(plot_images.items()):\n",
    "    rows, columns = 4, 5\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    idx = 0\n",
    "    for j in range(1, rows * columns + 1):\n",
    "        patch = images[idx]\n",
    "        idx += 1\n",
    "\n",
    "        plt.subplot(rows, columns, j)\n",
    "        plt.imshow(patch)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(classification_english[classification])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break down pose distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_by_class = {k: 0 for k in range(4)}\n",
    "\n",
    "for classification in classifications.values():\n",
    "    patches_by_class[classification] += 1\n",
    "\n",
    "print(f'=== Patches by class ===')\n",
    "for k, v in patches_by_class.items():\n",
    "    print(f'{classification_english[k]}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly plot 50 patches\n",
    "classifications = dict(sorted(classifications.items(), key=lambda x: np.random.rand()))\n",
    "\n",
    "plot_images = []\n",
    "for i, (patch_path, classification) in enumerate(classifications.items()):\n",
    "    if len(plot_images) >= 50:\n",
    "        break\n",
    "\n",
    "    patch = Image.open(patch_path)\n",
    "    plot_images.append(patch)\n",
    "\n",
    "rows, columns = 5, 10\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "idx = 0\n",
    "for i in range(1, rows * columns + 1):\n",
    "    plt.subplot(rows, columns, i)\n",
    "    plt.imshow(plot_images[idx])\n",
    "    plt.axis('off')\n",
    "    idx += 1\n",
    "\n",
    "plt.suptitle('1.3 - Training Data Selection', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While theoretically, the two command belows could be replaced for direct calls to the python train functions,\n",
    "in practice we call the bash scripts which request a CUDA GPU from slurm. We did not train or run inference from inside the Jupyter notebook as that would just bring redundant complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train unconditional model\n",
    "!bash ./train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train conditional model\n",
    "!bash ./train_conditional.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Video-Game Evaluation Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use larger model\n",
    "classification_model = YOLO('./pretrained-models/yolov8l.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_path = 'Data/Test/Test.mp4'\n",
    "patches_save_dir = 'TestDataset/patches'\n",
    "\n",
    "os.makedirs(patches_save_dir, exist_ok=True)\n",
    "\n",
    "dataset = create_frame_dataset(test_video_path, dim=(640, 640))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "metadata = []\n",
    "# For the test-video, we have to extract every patch with a human detected\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    frame_data = []\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patches = extract_human_patches(batch, lookback=[], filter=False)\n",
    "\n",
    "        for j, patch in enumerate(patches):\n",
    "            patch_box = patch['patch']\n",
    "            patch_coords = patch['coords']\n",
    "\n",
    "            save_image(patch_box, os.path.join(patches_save_dir, f'frame{i}_patch{j}.png'))\n",
    "\n",
    "            frame_data.append({\n",
    "                'coords': patch_coords,\n",
    "            })\n",
    "\n",
    "    metadata.append(frame_data)\n",
    "\n",
    "metadata_path = 'TestDataset/metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract segmentations\n",
    "save_dir = 'TestDataset/segmentations'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "dataset = create_patch_dataset(patches_save_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    detect_and_save_segmentation(batch, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_dir = 'TestDataset/patches'\n",
    "dataset = create_patch_dataset(patches_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    patch = batch['patch'][0]\n",
    "    patch_path = batch['patch_path'][0]\n",
    "    segmentation_path = patch_path.replace('patches', 'segmentations').replace('.png', '.pt')\n",
    "\n",
    "    if not os.path.exists(segmentation_path):\n",
    "        os.remove(patch_path)\n",
    "        continue\n",
    "\n",
    "    # === Open Segmentation ===\n",
    "    segmentation = torch.load(segmentation_path)\n",
    "    masks, conf = segmentation['masks'], segmentation['conf']\n",
    "    masks = masks.data\n",
    "    num_segmentations = masks.shape[0]\n",
    "\n",
    "    if num_segmentations == 0:\n",
    "        os.remove(patch_path)\n",
    "        continue\n",
    "\n",
    "    if num_segmentations != 1:\n",
    "        # Take biggest mask\n",
    "        mask_areas = [mask.sum().item() for mask in masks]\n",
    "        mask_idx = np.argmax(mask_areas)\n",
    "        mask = masks[mask_idx].unsqueeze(0)\n",
    "        conf = conf[mask_idx]\n",
    "    else:\n",
    "        mask = masks[0].unsqueeze(0)\n",
    "        conf = conf[0]\n",
    "\n",
    "    mask = mask.repeat(3, 1, 1)\n",
    "    masked_patch = patch * mask\n",
    "\n",
    "    # Save masked patch\n",
    "    save_image(masked_patch, patch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faces and poses for the test patches\n",
    "test_patches_dir = 'TestDataset/patches'\n",
    "test_faces_dir = 'TestDataset/faces'\n",
    "test_poses_dir = 'TestDataset/poses'\n",
    "\n",
    "os.makedirs(test_faces_dir, exist_ok=True)\n",
    "os.makedirs(test_poses_dir, exist_ok=True)\n",
    "\n",
    "dataset = create_patch_dataset(test_patches_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    detect_and_save_faces(batch, test_faces_dir)\n",
    "    detect_and_save_poses(batch, test_poses_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./inference.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./inference_conditional.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsquare_pad(img, target_size):\n",
    "    h, w = img.shape[:2]\n",
    "    target_h, target_w = target_size\n",
    "\n",
    "    pad_h = abs((target_h - h) // 2)\n",
    "    pad_w = abs((target_w - w) // 2)\n",
    "\n",
    "    # Trim to target size\n",
    "    img = img[pad_h:pad_h + target_h, pad_w:pad_w + target_w]\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch Game Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patches_dir = 'TestDataset/patches'\n",
    "test_video_path = 'Data/Test/Test.mp4'\n",
    "processed_frames_dir = 'TranslatedMovieOutput'\n",
    "segmentations_dir = 'TestDataset/segmentations'\n",
    "metadata_path = 'TestDataset/metadata.json'\n",
    "save_dir = processed_frames_dir + '/stitched_frames'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "dataset = create_frame_dataset(test_video_path, dim=(640, 640))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    frame = batch[0].permute(1, 2, 0).numpy()\n",
    "    frame = (frame * 255).astype(np.uint8)\n",
    "    frame_data = metadata[i]\n",
    "\n",
    "    for j, patch_data in enumerate(frame_data):\n",
    "        patch_path = f'{processed_frames_dir}/frame{i}_patch{j}.png'\n",
    "        mask_path = f'{segmentations_dir}/frame{i}_patch{j}.pt'\n",
    "\n",
    "        if not os.path.exists(patch_path) or not os.path.exists(mask_path):\n",
    "            continue\n",
    "\n",
    "        converted_patch = Image.open(patch_path)\n",
    "        converted_patch = np.array(converted_patch)\n",
    "\n",
    "        masks = torch.load(mask_path)['masks'].data\n",
    "        if masks.shape[0] != 1:\n",
    "            mask_areas = [mask.sum().item() for mask in masks]\n",
    "            mask_idx = np.argmax(mask_areas)\n",
    "            mask = masks[mask_idx]\n",
    "        else:\n",
    "            mask = masks[0]\n",
    "\n",
    "        mask = mask.unsqueeze(0).repeat(3, 1, 1).numpy().transpose(1, 2, 0)\n",
    "\n",
    "        # Resize mask and patch to original size\n",
    "        patch_coords = patch_data['coords']\n",
    "        x1, y1, x2, y2 = patch_coords\n",
    "        \n",
    "        # First thing we need to do is undo the resizing, but for this we need to know what we square padded it to\n",
    "        # thats easy, its just the max of the width and height\n",
    "        dim = max(x2 - x1, y2 - y1)\n",
    "\n",
    "        # resize to (dim, dim)\n",
    "        converted_patch = cv2.resize(converted_patch, (dim, dim))\n",
    "        mask = cv2.resize(mask, (dim, dim))\n",
    "\n",
    "        # undo the square padding by cropping to (x2 - x1, y2 - y1)\n",
    "        converted_patch = unsquare_pad(converted_patch, (y2 - y1, x2 - x1))\n",
    "        mask = unsquare_pad(mask, (y2 - y1, x2 - x1))\n",
    "\n",
    "        frame[y1:y2, x1:x2] = converted_patch * mask + frame[y1:y2, x1:x2] * (1 - mask)\n",
    "\n",
    "    # upscale\n",
    "    frame = Image.fromarray(frame)\n",
    "    frame = frame.crop(frame.getbbox())\n",
    "    frame = np.array(frame)\n",
    "    frame = cv2.resize(frame, (1080, 720))\n",
    "\n",
    "    # save processed frame\n",
    "    save_path = os.path.join(save_dir, f'frame{i}.png')\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile frames into video\n",
    "output_video_path = 'TranslatedMovieOutput/translated_movie.mp4'\n",
    "\n",
    "frame_paths = [os.path.join(save_dir, f) for f in os.listdir(save_dir)]\n",
    "frame_paths = sorted(frame_paths, key=lambda x: int(x.split('/')[-1].split('.')[0].split('frame')[-1]))\n",
    "\n",
    "frame = cv2.imread(frame_paths[0])\n",
    "height, width, _ = frame.shape\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, 30, (width, height))\n",
    "\n",
    "for frame_path in frame_paths:\n",
    "    frame = cv2.imread(frame_path)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [\n",
    "    '54',\n",
    "    '160',\n",
    "    '317',\n",
    "    '523',\n",
    "    '588',\n",
    "    '728',\n",
    "    '787',\n",
    "    '830',\n",
    "    '1723',\n",
    "    '1932',\n",
    "]\n",
    "\n",
    "rows, columns = 2, 10\n",
    "fig, axs = plt.subplots(rows, columns, figsize=(20, 5))\n",
    "# for each frame, plot both the one in ./TranslatedMovieOutput_Unconditional/stitched_frames and the one in ./TranslatedMovieOutput/stitched_frames\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    frame_unconditional_path = f'./TranslatedMovieOutput_Unconditional/stitched_frames/frame{frame}.png'\n",
    "    frame_conditional_path = f'./TranslatedMovieOutput/stitched_frames/frame{frame}.png'\n",
    "\n",
    "    frame_unconditional = cv2.imread(frame_unconditional_path)\n",
    "    frame_conditional = cv2.imread(frame_conditional_path)\n",
    "\n",
    "    axs[0, i].imshow(cv2.cvtColor(frame_unconditional, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, i].axis('off')\n",
    "\n",
    "    axs[1, i].imshow(cv2.cvtColor(frame_conditional, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, i].axis('off')\n",
    "\n",
    "# label first row\n",
    "for i in range(columns):\n",
    "    axs[0, i].set_title(f'Unconditional {frames[i]}')\n",
    "    axs[1, i].set_title(f'Conditional {frames[i]}')\n",
    "\n",
    "plt.suptitle('Comparison of Unconditional and Conditional Models', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
